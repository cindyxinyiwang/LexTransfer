\section{\label{sec:vocab_reduction}Lexical Representation for Multilingual NMT}
 To better utilize multilingual data, the encoding of lexical units from different but related languages should maximize information sharing through character level similarities while preserving language specific properties. With this assumption in mind, we examine the pros and cons of common methods of lexical representations in multilingual NMT, and explain the merits of our proposed framework.

\subsection{Fixed Size Word Vocabulary}
The simpliest approach is to map the most frequent words to individual embeddings, while the out-of-vocabulary words are mapped to a specific "unk" embedding. While this is a simple approach that allows the most flexibility for modeling the lexical units, it has some obvious limitations, particularly for multilingual data. First, with a fixed vocabulary size, having multiple languages can significantly increase the number of unknown words. Second, different languages, even related languages, have very few word-level overlap. Therefore, no lexical level information can be transferred through the word embedding. 

\subsection{Character Level NMT}
Fully character-level NMT maximizes the lexical level transfer between languages with same or similar character sets. However, almost no language specific information is preserved through character embedding. Moreover, character-level NMT has the particular challenges that it requires a larger model size to work well and training is usually much slower~\citep{fully_char_nmt,char_nmt_compression}.

\subsection{Byte Pair Encoding}
Byte Pair Encoding (BPE)~\citep{bpe} is a common preprocessing step in NMT to effectively reduce vocabulary size and remove unknown words. This can be viewed as a middle ground between fixed size word vocabulary and character vocabulary. For multilingual data, there are two approaches of using BPE. 
\paragraph{Joint BPE} learns a shared BPE vocabulary jointly on the multilingual data. This approach enables sharing of lexical representations between different languages, but it has some limitations: 1) the language specific properties of lexical units are completely lost; 2) BPE segmentation learned as a preprocessing step might not be optimal, as the word segments from high resource language might dominate the BPE vocabulary, so that the words in low resource language can be split into extremely small pieces or even characters. 
\paragraph{Seperate BPE} learns BPE vocabulary for each language separately and concatenate the results together. This approach is better at preserving language specific features of lexical units, but also has certain down sides: 1) a much larger vocabulary is needed for multilingual data; 2) lexical level transfer will be hard since the overlap of separate BPE vocabulary from different languages is limited.

\subsection{Soft Decoupling of Language Specific and Universal Information}
Given the conflict between lexical level transfer and preserving language specific properties, we propose a new framework for representing lexical units for multilingual NMT. Specifically, the representation of a lexical unit should be decomposed into semantic concept shared by all languages and features specific to the particular language. However, the decomposition should be carried out in a "\textit{soft}" manner, since the language specific features of a lexical unit, such as the way a word is spelled in each language, are often related to the universal concept or meaning it represents. For example, the word "color" in English is spelled similarly as "coleur" in French. Such a framework enables effective transfer of language universal meanings of lexical units while giving the model capacity to encode different forms of lexical units from different languages. We formalize the design of our model in the next section.  

%We utilize word encodings constructed from character-level information to facilitate the sharing of embedding space for multilingual vocabulary. We specify the building blocks of our method as follows:

%paragraph{Character Aware Word Embedding} ensures that lexical units with similar surface form are mapped to the same region in the embedding space. Formally, 

%\paragraph{Language Specific Transformation} preserves the language specific properties of the word embeddings constructed from character level information.

%\paragraph{Shared Latent Word Embedding} enables further transfer of lexical encoding between different languages with a relatively small-sized embedding space.  