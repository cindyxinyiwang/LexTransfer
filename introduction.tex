\section{\label{sec:introduction}Introduction}

\gn{Title: Try to avoid abbreviations such as NMT.}

%Transfering from related high-resourced languages to the low-resource language has shown great potential in improving the performance of low-resource Neural Machine Translation~(NMT).
Multilingual Neural Machine Translation~(NMT) has shown great potential both in the parameter-efficient creation of MT systems for many languages \cite{google_multi_nmt}, and for improving translation quality of low-resource languages~\citep{multi_nmt_adapt,universal_nmt,rapid_adapt_nmt,multi_nmt_shared_attn,multi_nmt_bpe_share}.
Despite the effectiveness of multilingual NMT training, existing transfer methods on multilingual data often requires limiting the transfer \gn{``limiting the transfer'' seems a little strange/vague here? using a transitive verb makes it sound like something we're doing intentionally.} of lexical level representations from different languages.
This is mainly because the standard sequence-to-sequence~(seq2seq) NMT model represents each lexical unit by a vector from a lookup table, which makes it hard to share across different languages due to limited overlap in their lexicons.
\gn{Maybe combine the following two sentences, mentioning them as alternative methods.}
To fine-tune a NMT model trained on another language, \cite{multi_nmt_adapt} randomly assigns vocabulary of the new language to the word embedding table of the pretrained NMT model.
To train a single model directly on multilingual data, separate word embeddings are often reserved for each language~\citep{rapid_adapt_nmt,google_multi_nmt,multi_nmt_share_enc}.
\gn{The explanation in the following sentence is vague. Given that the following is most related to ours, I think we need to explain it clearly. Also, the downsides we cite here don't seem like very big downsides. These are things that are easy to do. Can we make a stronger argument about what is good about our method?}
Recently, \cite{universal_nmt} propose a universal embedding space for better sharing of lexical-level information of multilingual NMT, but their method requires pretrained monolingual word embedding for each language and heuristic initialization through alignment.

\gn{I think there are three key insights behind our method: (1) character-based embedding is essential (unlike Gu et al, most other work), (2) subword-based embedding is sub-optimal (unlike Gu et al, other work), (3) just character-based embedding is not enough in multi-lingual settings so we need latent projection (inspired by Gu et al.). Maybe we can lay these all out explicitly, and maybe even include examples of places where this is a problem in our data? e.g. (1) rare words that share tons of characters but get mis-translated in the low-resource language,  (2) words in the LRL that get split up too much, (3) words where character-based embedding lead to mistranslation, but our model is able to fix this. I think having actual examples motivating our method would be good.}

In this paper, we propose a new representation framework of multilingual lexicons for better lexical-level transfer of NMT. We \textit{softly} decouple the traditional word embedding into two interacting components: one represents how the word is spelled, and the other is its meaning universally shared by all languages. We can view this representation as a decomposition of language specific features (i.e. spelling of a word) and language universial information. More importantly, our decoupling is done in a \textit{soft} manner to preserve the interaction between these two components. Concretely, our model applies a language specific transformation on the character embedding of a word built from a shared character embedding space, then the language specific character representation is used to query a latent word embedding space shared by all languages. Our method has the following desirable properties: 1) it enhances lexical-level transfer through the shared latent word embedding while preserving the model's capacity to learn specific features for each language; 2) it eliminates unknown word without any external preprocessing step like Byte Pair Encoding (BPE) \citep{bpe}; 3) it is able to utilize character-level information with much faster training speed than fully character-level NMT~\citep{}.

\gn{Add a short paragraph about results here.}
