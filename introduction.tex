\section{\label{sec:introduction}Introduction}

Multilingual Neural Machine Translation~(NMT) has shown great potential both in the parameter-efficient creation of MT systems for many languages \cite{google_multi_nmt}, and for improving translation quality of low-resource languages~\citep{multi_nmt_adapt,universal_nmt,rapid_adapt_nmt,multi_nmt_shared_attn,multi_nmt_bpe_share}.
Despite the effectiveness of multilingual NMT training, existing transfer methods on multilingual data often have trouble sharing the lexical level representations from different languages.
This is mainly because the standard sequence-to-sequence~(seq2seq) NMT model represents each lexical unit by a vector from a look-up table, which is hard to share across different languages with limited overlap in lexicons.
%\gn{Maybe combine the following two sentences, mentioning them as alternative methods.}

Two common approaches of translating multiple source languages to one single target language~(many-to-one translation) are training one model directly on the concatenated multilingual data~\citep{rapid_adapt_nmt,google_multi_nmt,multi_nmt_share_enc}, or adapting a model trained on a high-resource data to a low-resource data~\citep{multi_nmt_adapt}. In the first case, separate word embedding tables are reserved for each language; in the second case, the vocabulary of the new language is randomly assigned to the word embedding table of the pretrained model. Recently, \cite{universal_nmt} propose a universal embedding space for better sharing of lexical-level information of multilingual NMT. However, their method has several limitations:1) their monolingual embedding is trained on extra monolingual data, which are often limited or not available for low-resource languages; 2) it still needs to reserve separated word embedding for the most frequent words in each language; 3) although their method works with languages with distinct character vocabulary by pretraining monolingual embedding for each language, it cannot capture the character-level similarities of words from related languages. 

\gn{I think there are three key insights behind our method: (1) character-based embedding is essential (unlike Gu et al, most other work), (2) subword-based embedding is sub-optimal (unlike Gu et al, other work), (3) just character-based embedding is not enough in multi-lingual settings so we need latent projection (inspired by Gu et al.). Maybe we can lay these all out explicitly, and maybe even include examples of places where this is a problem in our data? e.g. (1) rare words that share tons of characters but get mis-translated in the low-resource language,  (2) words in the LRL that get split up too much, (3) words where character-based embedding lead to mistranslation, but our model is able to fix this. I think having actual examples motivating our method would be good.}

In this paper, we propose a new representation framework of multilingual lexicons for better lexical-level transfer of NMT. We \textit{softly} decouple the traditional word embedding into two interacting components: one represents how the word is spelled, and the other is its meaning universally shared by all languages. We can view this representation as a decomposition of language specific features (i.e. spelling of a word) and language universial information. More importantly, our decoupling is done in a \textit{soft} manner to preserve the interaction between these two components. Concretely, our model applies a language specific transformation on the character embedding of a word built from a shared character embedding space, then the language specific character representation is used to query a latent word embedding space shared by all languages. Our method has the following desirable properties: 1) it enhances lexical-level transfer through the shared latent word embedding while preserving the model's capacity to learn specific features for each language; 2) it eliminates unknown word without any external preprocessing step like Byte Pair Encoding (BPE) \citep{bpe}; 3) it is able to utilize character-level information with much faster training speed than fully character-level NMT~\citep{fully_char_nmt,char_nmt_compression}.

%\gn{Add a short paragraph about results here.}
We test our model on 4 low-resource languages from the TED corpus~\citep{ted_pretrain_emb}. Our method improves over the strong multilingual training baseline by over 2 BLEU without any preprocessing step like Byte Pair Encoding. Our experiments on various methods of lexical encodings show three insights toward multilingual NMT: 1) sub-word based embedding is sub-optimal; 2) character-based embedding is essential; 3) lexical-level sharing is helpful for transfer.