\section{\label{sec:introduction}Introduction}
%Transfering from related high-resourced languages to the low-resource language has shown great potential in improving the performance of low-resource Neural Machine Translation~(NMT).
Multilingual Neural Machine Translation~(NMT) has shown great potential in building efficient real world MT systems and improving translation quality of low-resource languages~\citep{google_multi_nmt,multi_nmt_adapt,universal_nmt,rapid_adapt_nmt,multi_nmt_shared_attn,multi_nmt_bpe_share}. Despite the effectiveness of multilingual NMT training, existing transfer methods on multilingual data often requires limiting the transfer of lexical level representations from different languages. This is mainly because the standard sequence-to-sequence~(seq2seq) NMT model represents each lexical unit by a vector from a lookup table, which is hard to share across different languages due to limited overlap in their lexicons. To fine-tune a NMT model trained on another language, \cite{multi_nmt_adapt} randomly assigns vocabulary of the new language to the word embedding table of the pretrained NMT model. To train a single model directly on multilingual data, separate word embeddings are often reserved for each language~\citep{rapid_adapt_nmt,google_multi_nmt,multi_nmt_share_enc}. Recently, \cite{universal_nmt} propose a universal embedding space for better sharing of lexical-level information of multilingual NMT, but their method requires pretrained monolingual word embedding for each language and heuristic initialization through alignment.

In this paper, we propose a new representation framework of multilingual lexicons for better lexical-level transfer of NMT. We \textit{softly} decouple the traditional word embedding into two interacting components: one represents how the word is spelled, and the other is its meaning universally shared by all languages. We can view this representation as a decomposition of language specific features (i.e. spelling of a word) and language universial information. More importantly, our decoupling is done in a \textit{soft} manner to preserve the interaction between these two components. Concretely, our model applies a language specific transformation on the character embedding of a word built from a shared character embedding space, then the language specific character representation is used to query a latent word embedding space shared by all languages. Our method has the following desirable properties: 1) it enhances lexical-level transfer through the shared latent word embedding while preserving the model's capacity to learn specific features for each language; 2) it eliminates unknown word without any external preprocessing step like Byte Pair Encoding (BPE) \citep{bpe}; 3) it is able to utilize character-level information with much faster training speed than fully character-level NMT~\citep{}.