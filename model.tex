\section{\label{sec:model}Model}
We assume a setting where we have $N$ languages, $L_1$, $L_2$, ..., $L_N$. For each word $w$ in each of these languages, we derive the language specific embedding $e_i(w)$. Our hypothesis is that every word $w$ can be "\textit{softly}" decoupled into two components through its character representation.  
\begin{itemize}
  \item The first source of representation, which we call \textit{lexical} meaning, is represented explicitly in the way $w$ is spelled, and thus is specific to languages but also weakly shared among languages. For instance, ``Christopher'', a common name in English, has the spelling ``Kry\v{s}tof'' in Czech. The two spellings are very different, yet they share some overlapping characters. Lexical meanings of words can be captured by looking at the character $n$-grams that form the word. The vocabulary of $n$-grams are also shared among different languages to capture the weak relationship between the similarities in the spellings of the word in different languages.
  \item The second source of meaning, which we call \textit{semantic} meaning, is language agnostic, i.e. it is the same in all languages. For instance, the concept of the cat is the same regardless of the English spelling ``cat'' or the French spelling ``chat''. While semantic meaning cannot be captured by looking at the word, there exists components in the word that can lead to some partial and helpful understanding. We thus propose to represent the (language agnostic) semantic meanings by querying a list of shared tokens using attention mechanism, where the keys are the lexical representations.
\end{itemize}

Given a word $w$ in a multilingual corpus from language $L_i$, we construct its embedding with the following steps.

\paragraph{Embedding from Characters} ensures that lexical units with similar surface form are mapped to the same region in the embedding space. It can be constructed in many different ways. Here we use the bag-of-ngram embedding proposed by . 
\begin{align}
    c(w) &= \text{tanh}(W_c \text{BON}(w))
\end{align}
where $\text{BON}$ projects a word into a vector of its character n-gram counts. Note that the $\text{BON}$ function and $W_c$ is shared among all languages.

\paragraph{Language Specific Encoding} preserves the language specific properties of the word embeddings constructed from character level information. It is applied to $c(w)$ by a linear transformation specific to the language $L_i$.
\begin{align}
    c_i(w) = W_i c(w)
\end{align}
Where $W_i$ is reserved only for words in $L_i$ and is not shared among different languages. 

\paragraph{Latent Word Embedding} enables further transfer of lexical encoding between different languages with a relatively small-sized embedding space. It is constructed by dot product attention \cite{} of $c_i(w)$ over a shared embedding space among languages.
\begin{align}
    e_{\text{latent}}(w) = \text{softmax}(W_s^{\top} c_i(w)) W_s
\end{align}
Where $W_s$ is the latent word embedding matrix shared among all languages.

Finally, to further facilitate character level lexical transfer between languages, we add the latent word embedding and character-level encoding together to produce the word embedding.
\begin{align}
    e(w) = e_{\text{latent}}(w) + c_i(w)
\end{align}
